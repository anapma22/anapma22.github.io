<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-10-24T18:34:24-03:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Ana Amarante</title><subtitle>Bacharel em Ciências e Tecnologia.    Graduanda em Eng. de Telecomunicações.     Especialista de TI - Redes e Infraestrutura.  </subtitle><author><name>Ana Amarante</name></author><entry><title type="html">NGINX com HTTPS no kubernetes</title><link href="http://localhost:4000/jekyll/update/2020/10/21/K8s-nfs-http.html" rel="alternate" type="text/html" title="NGINX com HTTPS no kubernetes" /><published>2020-10-21T23:06:38-03:00</published><updated>2020-10-21T23:06:38-03:00</updated><id>http://localhost:4000/jekyll/update/2020/10/21/K8s-nfs-http</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2020/10/21/K8s-nfs-http.html">&lt;p&gt;Este documento foi desenvolvido em &lt;strong&gt;abril de 2020&lt;/strong&gt;, durante a disciplina DevOps na Residência em tecnologia da informação aplicada à área jurídica - turma JF/IMD, na área de especialidade de Redes e Infraestrutura, ministrado pelo Prof Me. André Solino.&lt;/p&gt;

&lt;p&gt;O objetivo deste é ser um tutorial de instalação do HTTPS com o nginx. Os certificados serão gerados por meio do Let’s Encrypt.&lt;/p&gt;

&lt;p&gt;Este tutorial abordará os tópicos listados abaixo:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Definição teórica&lt;/li&gt;
  &lt;li&gt;Instalando o Cert-Manager&lt;/li&gt;
  &lt;li&gt;Implantando o emissor de produção&lt;/li&gt;
  &lt;li&gt;Configuração do yaml principal&lt;/li&gt;
  &lt;li&gt;Configuração do yaml referente ao certificado&lt;/li&gt;
  &lt;li&gt;Verificação do resultado&lt;/li&gt;
  &lt;li&gt;Referências&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;definição-teórica&quot;&gt;Definição teórica&lt;/h2&gt;
&lt;p&gt;O HTTPS é uma parte fundamental nos deploys de aplicações para  internet. Ele permite que as conexões entre cliente e servidor sejam encriptografadas. A sua instalação pode ser complexa, porém o Let’s Encrypt promove os certificados SSL/TLS e uma API para gerar esses certificados.&lt;/p&gt;

&lt;p&gt;O HTTPS alinhado ao Kubernetes nos permite especificar toda nossa aplicação, no contexto de réplicas, rede e certificados, tudo como IaC.&lt;/p&gt;

&lt;p&gt;A partir deste tutorial já temos o cluster configurado com o master e os workers, HAProxy e NFS configurado. Isso está explicado nos tutoriais anteriores.&lt;/p&gt;

&lt;h2 id=&quot;instalando-o-cert-manager&quot;&gt;Instalando o Cert-Manager&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Vamos instalar o cert-manager, ele é um serviço do Kubernetes que fornece certificados TLS gratuitos do Let’s Encrypt e outras autoridades certificadoras.&lt;/li&gt;
  &lt;li&gt;Antes de iniciar a instalação do cert-manager, é preciso criar um namespace que ele possa operar:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create namespace cert-manager
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Agora, vamos pegar a versão mais atualizada e estável do cert-manager com o yaml pronto. 
Todos os recursos (CustomResourceDefinitions, cert-manager, namespace, e webhook component) estão definidos aqui.
    &lt;ul&gt;
      &lt;li&gt;Caso queira verificar se já existe outra versão mais atualizada, entre neste &lt;a href=&quot;https://github.com/jetstack/cert-manager/releases&quot;&gt;link&lt;/a&gt; e verifique. Em caso positivo, troque o “0.14.2” pelo número da nova versão no comando abaixo.
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.14.2/cert-manager.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;A partir do comando anterior, os pods devem ter sido criados. Use o comando abaixo para verificar.
    &lt;ul&gt;
      &lt;li&gt;É normal que o pod cert-manager-webhook não inicie junto com os outros, pode levar de 2 a 3 minutos para que ele mude o status para Running. Isso acontece em razão dos ativos TLS necessários para o webhook funcionem sejam provisionados.
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get pods --namespace cert-manager
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/httpsk8s_1.png&quot; alt=&quot;Saída esperada do comando para verificação dos pods&quot; style=&quot;width:500px;height:90px;&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;Figura 1 - Saída esperada do comando para verificação dos pods. Fonte: Autoria própia &lt;/figcaption&gt;
&lt;/p&gt;

&lt;h2 id=&quot;implantando-o-emissor-de-produção&quot;&gt;Implantando o emissor de produção&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Antes de começar a emitir certificados, você deve configurar pelo menos um recurso Issuer, o emissor ou ClusterIssuer no seu cluster. Ele vai especificar a autoridade certificadora da qual será possível obter os certificados x509 assinados. 
Vamos criar o arquivo.
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vim prod_issuer.yaml 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Insira as definições abaixo no arquivo e salve em seguida. Se atente para o que está em negrito.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Será criado um objeto de ClusterIssuer chamado de letsencrypt-prod para usar o servidor de preparação do Let’s Encrypt. Será criado um Kubernetes Secret chamado letsencrypt-staging para armazenar a chave privada da conta ACME.&lt;/p&gt;

&lt;p&gt;Também habilitamos o mecanismo de desafio do HTTP-01. Segundo o site do  Let’s Encrypt, este é o tipo de desafio mais comum hoje em dia. O Let’s Encrypt fornece um token para o seu cliente ACME, e o cliente ACME coloca um arquivo no servidor web em http: // &lt;YOUR_DOMAIN&gt; /.well-known/acme-challenge/ &lt;TOKEN&gt;. Este arquivo contém o token, além de uma impressão digital da chave da sua conta.&lt;/TOKEN&gt;&lt;/YOUR_DOMAIN&gt;&lt;/p&gt;

&lt;p&gt;Depois que o cliente ACME informar o Let’s Encrypt que o arquivo está pronto, o  Let’s Crypt criptografará o arquivo. Se as verificações de validação obtiverem as respostas válidas do seu servidor web, a validação será considerada bem-sucedida e você poderá emitir seu certificado. Se as validações falharem, você precisará tentar novamente com um novo certificado.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: cert-manager.io/v1alpha2
kind: ClusterIssuer
metadata:
  name: letsencrypt-prod
  namespace: cert-manager
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory # The ACME server URL
    email: seu_email@test.com # Email address used for ACME registration
    privateKeySecretRef: # Name of a secret used to store the ACME account private key
      name: letsencrypt-prod #isso é pedido no ingress
    solvers: # Enable the HTTP-01 challenge provider
    - http01:
        ingress:
          class: haproxy #muda de acordo do ingress que você tiver configurado anteriormente
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Aplique o issuer configurado.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create -f prod_issuer.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;configuração-do-yaml-principal&quot;&gt;Configuração do yaml principal&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Agora vamos configurar o yaml que contém o deploy, service e ingress.
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vim lets-nginx.yaml 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Insira as definições abaixo e salve o arquivo.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment #deploy the Ingress Controller pod
metadata:
  labels:
    run: nginx
  name: lets-nginx
#  namespace: default #which to place the resources
spec:
  replicas: 1
  selector:
    matchLabels:
      run: nginx
  template:
    metadata:
      labels:
        run: nginx
    spec:
      volumes:
      - name: nfs1
        persistentVolumeClaim:
          claimName: nfs-pvc
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
        ports:
        - containerPort: 80
          protocol: TCP
        volumeMounts:
        - name: nfs1 #mesmo nome do volume criado neste deploy
          mountPath: /usr/share/nginx/html #path que será criado dentro do container
#---
apiVersion: v1
kind: Service #allow traffic to reach the Ingress Controller
metadata:
  labels:
    run: nginx
  name: lets-nginx #nome do service que deve ter o mesmo valor que serviceName no Ingress, caso contrário o Endpoint não sera encontrado #namespace: default #which to place the resources
spec:
  selector:
    run: nginx
  ports:
    - name: http
      port: 80
      targetPort: 80
#---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: nginx
  annotations:
   # kubernetes.io/ingress.class: &quot;haproxy&quot;
    cert-manager.io/cluster-issuer: &quot;letsencrypt-prod&quot; #foi setado no  prod_issuer.yaml
    certmanager.k8s.io/acme-challenge-type: http01
#  namespace: cert-manager
spec:
  tls:
  - hosts:
    - nginx-lets.34.66.16.93.nip.io  #altere com seu IP
    secretName: lets-tls
  rules:
  - host: nginx-lets.34.66.16.93.nip.io #altere com seu IP
    http:
      paths:
      - path: /
        backend:
          serviceName: lets-nginx
          servicePort: 80
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Aplique as alterações:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl apply -f lets-nginx.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;configuração-do-yaml-referente-ao-certificado&quot;&gt;Configuração do yaml referente ao certificado&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Por fim e não menos importante, vamos alterar o arquivo chave para o certificado.
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vim cert.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Insira o que está abaixo e salve o arquivo.
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: cert-manager.io/v1alpha2
kind: Certificate
metadata:
  name: certificate-nginx
  namespace: default
spec:
  secretName: nginx-tls
  duration: 24h
  renewBefore: 12h
  commonName: nginx-lets.34.66.16.93.nip.io #altere para seu IP
  dnsNames:
  - nginx-lets.34.66.16.93.nip.io  #altere para seu domínio
  issuerRef:
 name: letsencrypt-prod
 kind: ClusterIssuer
apiVersion: cert-manager.io/v1alpha2
kind: Certificate
metadata:
  name: certificate-nginx
  namespace: cert-manager
spec:
  secretName: nginx-tls
  duration: 24h
  renewBefore: 12h
  commonName: nfs-nginx.35.226.108.169.nip.io #altere para seu IP
  dnsNames:
  - nginx-lets.34.66.16.93.nip.io #altere para seu domínio
  issuerRef:
 name: letsencrypt-prod
 kind: ClusterIssuer
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;Aplique as alterações:&lt;/p&gt;
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create -f cert.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;h2 id=&quot;verificação-do-resultado&quot;&gt;Verificação do resultado&lt;/h2&gt;
    &lt;p&gt;Pode demorar alguns minutos, mas você verá que o certificado foi criado corretamente. Primeiro vamos verificar o clusterissuers:&lt;/p&gt;
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get clusterissuers
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/httpsk8s_2.png&quot; alt=&quot;Saída esperada do comando para verificação dos clusterissuers&quot; style=&quot;width:240px;height:60px;&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;Figura 2 - Saída esperada do comando para verificação dos clusterissuers. Fonte: Autoria própia &lt;/figcaption&gt;
&lt;/p&gt;
&lt;p&gt;Agora, vamos verificar o certificaterequest. A FIgura 3 é o final do comando abaixo:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl describe certificaterequest
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/httpsk8s_3.png&quot; alt=&quot;Saída esperada do  final do comando para verificação do certificaterequest&quot; style=&quot;width:500px;height:70px;&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;Figura 3 - Saída esperada do  final do comando para verificação do certificaterequest. Fonte: Autoria própia &lt;/figcaption&gt;
&lt;/p&gt;

&lt;p&gt;Após isso, você já pode ir no seu host e verificar se o acesso acontece com HTTPS por meio de um certificado válido.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/httpsk8s_4.png&quot; alt=&quot;Host setado nos yamls acessando com HTTPS&quot; style=&quot;width:700px;height:400px;&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;Figura 4 - Host setado nos yamls acessando com HTTPS. Fonte: Autoria própia &lt;/figcaption&gt;
&lt;/p&gt;

&lt;h4 id=&quot;referências&quot;&gt;Referências&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Como configurar um Nginx Ingress com Cert-Manager no Kubernetes da plataforma DigitalOcean. Disponível em: &lt;a href=&quot;https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nginx-ingress-with-cert-manager-on-digitalocean-kubernetes-pt&quot;&gt;https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nginx-ingress-with-cert-manager-on-digitalocean-kubernetes-pt&lt;/a&gt;. Acessado em: 19 abr. 2020.&lt;/li&gt;
  &lt;li&gt;Kubernetes. cert-manager. Disponível em: &lt;a href=&quot;https://cert-manager.io/docs/installation/kubernetes/&quot;&gt;https://cert-manager.io/docs/installation/kubernetes/&lt;/a&gt;. Acessado em: 19 abr. 2020.&lt;/li&gt;
  &lt;li&gt;HTTP01. cert-manager. Disponível em: &lt;a href=&quot;https://letsencrypt.org/docs/challenge-types/#http-01-challenge&quot;&gt;https://letsencrypt.org/docs/challenge-types/#http-01-challenge&lt;/a&gt;. Acessado em: 19 abr. 2020.&lt;/li&gt;
  &lt;li&gt;Challenge Types. Let’s Encrypt. Disponível em: &amp;lt;https://letsencrypt.org/docs/challenge-types/#http-01-challenge. Acessado em: 19 abr. 2020.&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Ana Amarante</name></author><summary type="html">Este documento foi desenvolvido em abril de 2020, durante a disciplina DevOps na Residência em tecnologia da informação aplicada à área jurídica - turma JF/IMD, na área de especialidade de Redes e Infraestrutura, ministrado pelo Prof Me. André Solino.</summary></entry><entry><title type="html">NFS com NGINX no kubernetes</title><link href="http://localhost:4000/jekyll/update/2020/10/21/K8s-nfs.html" rel="alternate" type="text/html" title="NFS com NGINX no kubernetes" /><published>2020-10-21T20:06:38-03:00</published><updated>2020-10-21T20:06:38-03:00</updated><id>http://localhost:4000/jekyll/update/2020/10/21/K8s-nfs</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2020/10/21/K8s-nfs.html">&lt;p&gt;Este documento foi desenvolvido em &lt;strong&gt;março de 2020&lt;/strong&gt;, durante a disciplina DevOps na Residência em tecnologia da informação aplicada à área jurídica - turma JF/IMD, na área de especialidade de Redes e Infraestrutura, ministrado pelo Prof Me. André Solino.&lt;/p&gt;

&lt;p&gt;O objetivo deste tutorial é a instalação do NFS no Kubernetes.&lt;/p&gt;

&lt;p&gt;Este tutorial abordará os tópicos listados abaixo:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Definição teórica&lt;/li&gt;
  &lt;li&gt;NFS server&lt;/li&gt;
  &lt;li&gt;NFS client&lt;/li&gt;
  &lt;li&gt;NFS no kubernetes&lt;/li&gt;
  &lt;li&gt;HAProxy Ingress Controller&lt;/li&gt;
  &lt;li&gt;Deploy do NGINX&lt;/li&gt;
  &lt;li&gt;Erros cometidos durante este trabalho&lt;/li&gt;
  &lt;li&gt;Referências&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;definição-teórica&quot;&gt;Definição teórica&lt;/h2&gt;
&lt;p&gt;Conforme está dito na documentação oficial do Kubernets: Um volume nfs permite que um compartilhamento NFS (Network File System) existente seja montado no seu Pod. O conteúdo de um volume nfs é preservado e o volume é apenas desmontado. Isso significa que um volume NFS pode ser preenchido previamente com dados e que esses dados podem ser “transferidos” entre os Pods. O NFS pode ser montado por vários gravadores simultaneamente.&lt;/p&gt;

&lt;p&gt;O Kubernetes usa o service como uma maneira abstrata de expor um aplicativo em execução em um conjunto de pods como um serviço de rede. O que faremos aqui será a configuração de um service para um volume persistente ou PersistentVolume (PV).&lt;/p&gt;

&lt;p&gt;É interessante observar também outro conceito de volumes que utilizaremos neste tutorial, que é o de PersistentVolumeClaim (PVC). Ele é usado para montar um PV dentro de um pod, já que permite que o PV seja utilizado de forma a independer de qualquer pod, isso alinhado ao NFS Server nos permite trabalhar com os arquivos de maneira a não se preocupar com a efemeridade dos pods.&lt;/p&gt;

&lt;h2 id=&quot;nfs-server&quot;&gt;NFS server&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;A máquina que será hospedado o NFS Server pode ser o master ou pode ser uma máquina exclusiva para esse servidor, o último caso é o mais indicado. 
Neste tutorial, utilizamos o centos7. Com uma máquina com disco de 10GB e vCPUI.&lt;/li&gt;
  &lt;li&gt;Instalação do NFS:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;yum install nfs-utils -y
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;h3 id=&quot;diretório-a-ser-compartilhado&quot;&gt;Diretório a ser compartilhado&lt;/h3&gt;
  &lt;/li&gt;
  &lt;li&gt;Criação do diretório que será compartilhado pelo NFS:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mkdir /var/nfsshare
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Definir permissões, no caso do &lt;em&gt;chmod&lt;/em&gt; será atribuído a permissão de leitura, escrita e execução para o dono da pasta (7), leitura e execução para usuários do mesmo grupo (5) e também para outros usuários (5). Já o chown, define que  &lt;em&gt;/var/nfsshare&lt;/em&gt; pertence a &lt;em&gt;nfsnobody&lt;/em&gt; e ao grupo &lt;em&gt;nfsnobody&lt;/em&gt;.
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;chmod -R 755 /var/nfsshare
chown nfsnobody:nfsnobody /var/nfsshare
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;ul&gt;
      &lt;li&gt;Caso queria compartilhar outro diretório como /home, as permissões deste diretório não devem ser alteradas.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Configurar os serviços, conforme abaixo:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;systemctl enable rpcbind
systemctl enable nfs-server
systemctl enable nfs-lock
systemctl enable nfs-idmap
systemctl start rpcbind
systemctl start nfs-server
systemctl start nfs-lock
systemctl start nfs-idmap
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Agora, compartilharemos o diretório NFS pela rede da seguinte maneira:
    &lt;ul&gt;
      &lt;li&gt;Abrir o arquivo que será responsável por setar as máquinas que têm acesso à partilha.
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vim /etc/exports
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;Criaremos um ponto de compartilhamento e &lt;em&gt;/var/nfsshare&lt;/em&gt;. Edite o arquivo &lt;em&gt;/etc/exports&lt;/em&gt; da seguinte maneira:
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/var/nfsshare    *(rw,sync,no_root_squash,no_all_squash)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;O * representa que a pasta &lt;em&gt;/var/nfsshare&lt;/em&gt; ficará acessível para qualquer cliente, caso queira definir os clientes, pode inserir os IPs nesse arquivo.&lt;/li&gt;
      &lt;li&gt;Para conhecimento, os parâmetros estão detalhados abaixo:&lt;/li&gt;
    &lt;/ul&gt;
    &lt;ul&gt;
      &lt;li&gt;rw → Permite leitura e escrita;&lt;/li&gt;
      &lt;li&gt;sync→ NFS Responde às solicitações somente após as alterações terem sido confirmadas no armazenamento estável;&lt;/li&gt;
      &lt;li&gt;no_root_squash → Uma técnica para anular a escalação de privilégios na máquina cliente.&lt;/li&gt;
      &lt;li&gt;no_all_squash → Similar ao no_root_squash, mas para usuários não roots.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Podemos reiniciar o NFS:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;systemctl restart nfs-server
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Configurar o firewall:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;firewall-cmd --permanent --zone=public --add-service=nfs
firewall-cmd --permanent --zone=public --add-service=mountd
firewall-cmd --permanent --zone=public --add-service=rpc-bind
firewall-cmd --reload
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;nfs-client&quot;&gt;NFS client&lt;/h2&gt;
&lt;p&gt;Essa configuração é feita em cada máquina.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Instalar o NF:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;yum install nfs-utils -y
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Criação do diretório que será compartilhado pelo NFS:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mkdir -p /mnt/nfs/var/nfsshare
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;O próximo passo será montar os diretórios compartilhados na máquina do cliente, como mostrado abaixo:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mount -t nfs 10.128.0.6:/var/nfsshare /mnt/nfs/var/nfsshare/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Agora que o NFS já foi conectado é hora de fazer uma verificação cruzada:&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;df -kh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ol&gt;
  &lt;li&gt;Checar as permissões de leitura e escrita por meio da inserção de um arquivo:&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;touch /mnt/nfs/var/nfsshare/test_nfs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Após isso verifique no server se o arquivo foi criado.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Para evitar que tenhamos que montar novamente o compartilhamento NFS no cliente após cada reinicialização. Podemos configurar, mas que ele fique permanentemente, adicionando o compartilhamento NFS no /etc/fstab, conforme está mostrado abaixo.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vim /etc/fstab
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ol&gt;
  &lt;li&gt;Inserir os parâmetros no arquivo e salvar:&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[...]
10.128.0.6:/var/nfsshare   /mnt/nfs/var/nfsshare   nfs defaults 0 0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Onde o IP inserido é o do NFS server.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Agora, podemos reiniciar a máquina para finalizar a configuração e garantir um ponto de montagem permanente do NFS.
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;reboot
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;nfs-no-kubernetes&quot;&gt;NFS no kubernetes&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Agora, que o NFS Server e Client já está configurado, temos duas opções para montar o compartilhamento existente.
    &lt;ul&gt;
      &lt;li&gt;Na primeira, o compartilhamento do NFS é configurado junto com a definição dos pods. Nesta opção o compartilhamento é manual, por isso não trabalharemos com ela.&lt;/li&gt;
      &lt;li&gt;Na segunda, o compartilhamento é definido como um objeto cluster por meio de PV, onde o cluster define um intervalo de compartilhamento do NFS para os inquilinos solicitarem e consumirem. Usaremos esta abordagem.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Nos yamls abaixo será abordado sobre dois conceitos, &lt;em&gt;accessModes&lt;/em&gt; e &lt;em&gt;persistentVolumeReclaimPolicy&lt;/em&gt;.
Modos de acessos:
    &lt;ul&gt;
      &lt;li&gt;ReadWriteOnce – o volume será montado como read-write por um único node.&lt;/li&gt;
      &lt;li&gt;ReadOnlyMany –  o volume será montado como read-only por vários nodes.&lt;/li&gt;
      &lt;li&gt;ReadWriteMany –  o volume será montado como read-write por vários nodes.
Reclaim é a política usada para definir o que o cluster deve fazer depois que um PV for liberado de um claim.&lt;/li&gt;
      &lt;li&gt;Retain - permite a recuperação manual do recurso. Quando o PVC é excluído, o PV ainda existe e o volume é considerado “liberado”.&lt;/li&gt;
      &lt;li&gt;Delete - para plug-ins de volume que oferecem suporte à política de recuperação. A exclusão remove o objeto PV do Kubernetes, bem como o ativo de armazenamento associado na infraestrutura externa (e.g. Azure, AWS, GCE ou Cinder).&lt;/li&gt;
      &lt;li&gt;Retain (deprecated) - executa uma limpeza básica (rm -rf / volume / *) no volume e o torna disponível novamente para uma nova reivindicação.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;expondo-compartilhamentos-nfs-como-um-objeto-de-cluster&quot;&gt;Expondo compartilhamentos NFS como um objeto de cluster&lt;/h3&gt;

&lt;h4 id=&quot;pv&quot;&gt;PV&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Neste método, usaremos os conceitos expostos anteriormente de PV e PVC, eles serão responsáveis por gerenciar e liberar o acesso ao compartilhamento do NFS.&lt;/li&gt;
  &lt;li&gt;Criar o arquivo para definição do PV à nível do cluster.
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vim nfs-pv.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Inserir o yaml abaixo no arquivo e salvar.
```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv # Altere com o nome que deseja para o PV
  labels:
 name: nfs-pv 
spec:
  storageClassName: manual
  capacity:
 storage: 300Mi # Altere para ser o tamanho do volume a ser compartilhado
  accessModes:
    &lt;ul&gt;
      &lt;li&gt;ReadWriteMany # Definição dos acessos
  persistentVolumeReclaimPolicy: Recycle # Política do reclaim 
  nfs:
 server: 10.128.0.6 # Altere com o IP do seu NFS server
 path: “/var/nfsshare” # Altere para o local de sua preferência
```&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Aplicar o pv-nfs.yaml:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl apply -f nfs-pv.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Verificar se o volume foi criado corretamente:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get pv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;pvc&quot;&gt;PVC&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Criar o arquivo para definição do PV à nível do cluster.
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vim nfs-pvc.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Inserir o yaml abaixo no arquivo e salvar.
```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-pvc # Altere com o nome que deseja para o PVC
spec:
  storageClassName: manual
  accessModes:
    &lt;ul&gt;
      &lt;li&gt;ReadWriteMany # Definição dos acessos
  resources:
 requests:
storage: 150Mi # Tamanho do volume requisitado
```&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Aplicar o pv-nfs.yaml:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl apply -f nfs-pvc.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Verificar se o volume foi criado corretamente:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get pvc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Verificar se o pv mudou de status Available para Bound:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get pv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;haproxy-ingress-controller&quot;&gt;HAProxy Ingress Controller&lt;/h2&gt;
&lt;p&gt;É esperado que o kubernetes trabalhe com um tráfego considerável, além de se tratar de um ambiente ser altamente dinâmico, devido a criação, deletamento e realocação de pods. Para isso utilizamos o HAProxy.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Crie os recursos:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create -f https://haproxy-ingress.github.io/resources/haproxy-ingress.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;O controlador ainda não está em execução. Hora de editar qualquer valor padrão, por exemplo, a versão da imagem do controlador:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl -n ingress-controller edit configmap haproxy-ingress
kubectl -n ingress-controller edit daemonset haproxy-ingress
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Rotule pelo menos um nó:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl label node master-1 role=ingress-controller
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Agora o HAProxy Ingress deve estar em funcionamento:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl -n ingress-controller get pod
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/nfs_1.png&quot; alt=&quot;Saída esperada com o HAProxy Ingress up&quot; style=&quot;width:320px;height:50px;&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;Figura 1 - Saída esperada com o HAProxy Ingress up. Fonte: Autoria própia &lt;/figcaption&gt;
&lt;/p&gt;

&lt;h2 id=&quot;deploy-do-nginx&quot;&gt;Deploy do NGINX&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Criar o arquivo nfs-nginx.yaml:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vim nfs-nginx.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Inserir no arquivo e salvar:&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment # Deploy do pod
metadata:
  labels:
    run: nginx
  name: nfs-nginx
  namespace: default 
spec: 
  replicas: 1 
  selector:
    matchLabels:
      run: nginx
  template:
    metadata:
      labels:
        run: nginx
    spec:
      volumes:
      - name: nfs1 # Nome do volume
        persistentVolumeClaim:
          claimName: nfs-pvc # Nome do pvc criado anteriormente
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
        ports:
        - containerPort: 80
          protocol: TCP
        volumeMounts:
        - name: nfs1 # Mesmo nome do volume criado neste deploy
          mountPath: /usr/share/nginx/html # Path que será criado dentro do container    
#---
apiVersion: v1
kind: Service # Permite que o tráfego seja passado para o Ingress Controller
metadata:
  labels:
    run: nginx
  name: nfs-nginx # Nome do service que deve ter o mesmo valor que serviceName no Ingress, caso contrário o Endpoint não será encontrado
 spec:
  selector:
    run: nginx
  ports:
    - name: http
      port: 80
      targetPort: 80
#---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: nginx # Altere com o nome do ingress
  namespace: default
spec:
  rules:
  - host: nfs-nginx.35.222.201.213.nip.io # Altere com o seu host nip.io para mais informações
    http:
      paths:
      - path: /
        backend:
          serviceName: nfs-nginx # Altere com o nome do service 
          servicePort: 80
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;Aplique o yaml criado:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl apply -f nfs-nginx.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Após isso vamos inserir o index.html dentro do NFS Server (&lt;em&gt;/var/nfsshar&lt;/em&gt;), pois essa pasta foi mapeada para dentro dos pods (&lt;em&gt;/usr/share/nginx/html&lt;/em&gt;):
    &lt;ul&gt;
      &lt;li&gt;Entre no NFS Server e vá para o diretório &lt;em&gt;/var/nfsshar&lt;/em&gt;:
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vim index.html
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;Coloque alguma informação no arquivo e salve.&lt;/li&gt;
      &lt;li&gt;Ao acessar o endereço salvo no ingress, é possível vê a página, como esperado:&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/nfs_2.png&quot; alt=&quot; index.html no NFS Server&quot; style=&quot;width:200px;height:70px;&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;Figura 2 -  index.html no NFS Server. Fonte: Autoria própia &lt;/figcaption&gt;
&lt;/p&gt;

&lt;p&gt;É possível verificar no worker, o mapeamento feito pelo NFS:&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/nfs_3.png&quot; alt=&quot;Mapeamento do NFS no worker,&quot; style=&quot;width:900px;height:70px;&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;Figura 3 - Mapeamento do NFS no worker, sem nada ter sido configurado lá. Fonte: Autoria própia &lt;/figcaption&gt;
&lt;/p&gt;

&lt;p&gt;Como foi visto, o arquivo criado dentro do NFS Server foi lido pelos pods e a página foi exibida corretamente.&lt;/p&gt;

&lt;h2 id=&quot;erros-cometidos-durante-este-trabalho&quot;&gt;Erros cometidos durante este trabalho&lt;/h2&gt;
&lt;p&gt;O objetivo desta seção é apontar os erros que enfrentei, caso você passe por eles já saberá a solução facilmente.&lt;/p&gt;

&lt;p&gt;O nfs-nginx.yaml (com o deploy, service e ingress) estava com a identação incorreta e inicialmente a declaração dos volumes no lugar errado, dentro dos containers (o que é dentro do containers é o volumeMounts). O erro aconteceu por falta de atenção e a solução foi tomada em etapas, verifiquei que tinha algo de errado com os volumes, pois ao comentar apenas aquela parte o nginx funcionava.&lt;/p&gt;

&lt;p&gt;Outro erro foi acreditar que o index.html deveria está dentro dos pods. Estava entrando nos pods e colocava o arquivo dentro /usr/share/nginx/html, porém percebi que isto não fazia sentido, visto que os pods são efêmeros, então percebi que o local correto seria no NFS Server que é na pasta mapeada.&lt;/p&gt;

&lt;h4 id=&quot;referências&quot;&gt;Referências&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;NFS Server and Client Installation on CentOS 7. How to Forge. Disponível em: &lt;a href=&quot;https://www.howtoforge.com/nfs-server-and-client-on-centos-7&quot;&gt;https://www.howtoforge.com/nfs-server-and-client-on-centos-7&lt;/a&gt;. Acessado em: 24 mar. 2020.&lt;/li&gt;
  &lt;li&gt;Volumes. Kubernetes. Disponível em: &lt;a href=&quot;https://kubernetes.io/docs/concepts/storage/volumes/&quot;&gt;https://kubernetes.io/docs/concepts/storage/volumes/&lt;/a&gt;. Acessado em: 05 abr. 2020.&lt;/li&gt;
  &lt;li&gt;Service. Kubernetes. Disponível em: &lt;a href=&quot;https://kubernetes.io/docs/concepts/services-networking/service/&quot;&gt;https://kubernetes.io/docs/concepts/services-networking/service/&lt;/a&gt;. Acessado em: 05 abr. 2020.&lt;/li&gt;
  &lt;li&gt;Configuring NFS Storage for Kubernetes. Docker Docs. Disponível em: &lt;a href=&quot;https://docs.docker.com/ee/ucp/kubernetes/storage/use-nfs-volumes/&quot;&gt;https://docs.docker.com/ee/ucp/kubernetes/storage/use-nfs-volumes/&lt;/a&gt;. Acessado em: 05 abr. 2020.&lt;/li&gt;
  &lt;li&gt;Kubernetes/example/nfs. Github. Disponível em: &lt;a href=&quot;https://github.com/kubernetes/examples/tree/master/staging/volumes/nfs&quot;&gt;https://github.com/kubernetes/examples/tree/master/staging/volumes/nfs&lt;/a&gt;. Acessado em: 05 abr. 2020.&lt;/li&gt;
  &lt;li&gt;HAPROXY INGRESS. Getting Started. Disponível em: &lt;a href=&quot;https://haproxy-ingress.github.io/docs/getting-started/&quot;&gt;https://haproxy-ingress.github.io/docs/getting-started/&lt;/a&gt;. Acessado em: 08 abr. 2020.&lt;/li&gt;
  &lt;li&gt;nip.io. Disponível em: &lt;a href=&quot;https://nip.io/&quot;&gt;https://nip.io/&lt;/a&gt;. Acessado em: 08 abr. 2020.&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Ana Amarante</name></author><summary type="html">Este documento foi desenvolvido em março de 2020, durante a disciplina DevOps na Residência em tecnologia da informação aplicada à área jurídica - turma JF/IMD, na área de especialidade de Redes e Infraestrutura, ministrado pelo Prof Me. André Solino.</summary></entry><entry><title type="html">Instalação do Kubernetes</title><link href="http://localhost:4000/jekyll/update/2020/10/20/K8s-Instalacao.html" rel="alternate" type="text/html" title="Instalação do Kubernetes" /><published>2020-10-20T20:06:38-03:00</published><updated>2020-10-20T20:06:38-03:00</updated><id>http://localhost:4000/jekyll/update/2020/10/20/K8s-Instalacao</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2020/10/20/K8s-Instalacao.html">&lt;p&gt;Este documento foi desenvolvido em &lt;strong&gt;março de 2020&lt;/strong&gt;, durante a disciplina DevOps na Residência em tecnologia da informação aplicada à área jurídica - turma JF/IMD, na área de especialidade de Redes e Infraestrutura, ministrado pelo Prof Me. André Solino.&lt;/p&gt;

&lt;p&gt;O objetivo deste tutorial é a instalação do Kubernetes.&lt;/p&gt;

&lt;p&gt;Este tutorial abordará os tópicos listados abaixo:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Criação das instâncias&lt;/li&gt;
  &lt;li&gt;Container runtimes  - Docker&lt;/li&gt;
  &lt;li&gt;Iniciando Clusters com o kubeadm&lt;/li&gt;
  &lt;li&gt;Comandos interessantes&lt;/li&gt;
  &lt;li&gt;Referências&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;criação-das-instâncias&quot;&gt;Criação das instâncias&lt;/h2&gt;

&lt;p&gt;Foram criadas três máquinas, onde uma será o master e as outras duas os workers.
Todas elas com 2vCPU e o centOS7.
Para facilitar o trabalho, em casos de comandos repetidos usamos o tilix para digitar nos três terminais de uma vez.
Todas foram logadas como root.&lt;/p&gt;

&lt;h2 id=&quot;container-runtimes----docker&quot;&gt;Container runtimes  - Docker&lt;/h2&gt;

&lt;p&gt;Para usar os containers nos pods, o K8s usa alguns Container runtimes, em nosso caso será o Docker, então, vamos para sua instalação. 
&lt;strong&gt;Esse processo deve ser feito em todas as máquinas!&lt;/strong&gt;
Logue como root: sudo su -&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Install Docker CE
## Set up the repository
### Install required packages.
yum install -y yum-utils device-mapper-persistent-data lvm2

### Add Docker repository.
yum-config-manager --add-repo \
  https://download.docker.com/linux/centos/docker-ce.repo

## Install Docker CE.
yum update -y &amp;amp;&amp;amp; yum install -y \
  containerd.io-1.2.10 \
  docker-ce-19.03.4 \
  docker-ce-cli-19.03.4

## Create /etc/docker directory.
mkdir /etc/docker

# Setup daemon.
cat &amp;gt; /etc/docker/daemon.json &amp;lt;&amp;lt;EOF
{
  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],
  &quot;log-driver&quot;: &quot;json-file&quot;,
  &quot;log-opts&quot;: {
    &quot;max-size&quot;: &quot;100m&quot;
  },
  &quot;storage-driver&quot;: &quot;overlay2&quot;,
  &quot;storage-opts&quot;: [
    &quot;overlay2.override_kernel_check=true&quot;
  ]
}
EOF

mkdir -p /etc/systemd/system/docker.service.d

# Restart Docker
systemctl daemon-reload

systemctl restart docker

systemctl enable docker
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;iniciando-clusters-com-o-kubeadm&quot;&gt;Iniciando Clusters com o kubeadm&lt;/h2&gt;

&lt;p&gt;Como já foi dito nos outros documentos, o kubeadm  automatiza parte do processo de criação do cluster.&lt;/p&gt;

&lt;p&gt;Como estamos trabalhando em um laboratório para ambiente de produção, iremos parar e desabilitar o firewalld. Caso você esteja em um ambiente de produção real é importante rever a documentação deste serviço para que ele fique habilitado e não abra falhas de segurança na sua implementação.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Parando o firewalld:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;systemctl stop firewalld
systemctl disable firewalld
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Portas que devem estar liberadas:
    &lt;ul&gt;
      &lt;li&gt;Master: 6443*, 2379-2380, 10250, 10251 e 10252 .&lt;/li&gt;
      &lt;li&gt;Workers: 10250 e 30000-32767.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;instalação-do-kubeadm-kubelet-and-kubectl&quot;&gt;Instalação do kubeadm, kubelet and kubectl&lt;/h3&gt;
&lt;p&gt;Como dito acima, o &lt;strong&gt;kubeadm&lt;/strong&gt; automatiza parte do processo de criação do cluster. O &lt;strong&gt;kubelet&lt;/strong&gt; faz a interface com o docker, atua como agente em cada node e o &lt;strong&gt;kubectl&lt;/strong&gt; é a interface de linha de comando do K8s.
&lt;strong&gt;Esse processo deve ser feito em todas as máquinas!&lt;/strong&gt;
É necessário definir o SELinux no modo permissivo executando os comando abaixo. Isso irá acontecer até que o suporte ao SELinux seja aprimorado no kubelet.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

# Set SELinux in permissive mode (effectively disabling it)
setenforce 0
sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes

systemctl enable --now kubelet
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Alguns usuários do RHEL / CentOS 7 relataram problemas com o tráfego sendo roteado incorretamente devido ao desvio do iptables. Você deve garantir que net.bridge.bridge-nf-call-iptables esteja definido como 1 na sua configuração sysctl, por exemplo:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Para ter certeza que o módulo br_netfilter foi um módulo carregado no kernerl, utilizae o lsmod que é um comando para sistemas operacionais Linux que exibe quais módulos carregáveis do kernel estão atualmente carregados.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;lsmod | grep br_netfilter
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Para que você fique com a versão mais atualizada do kubeadm, rode:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;yum update
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Aqui, caso seja mostrado alguma atualização do docker, você não deve marcar para atualizar os pacotes: &lt;strong&gt;docker-ce, docker-ce-cli ou containerd.io&lt;/strong&gt;. O repositório do docker atualiza mais rápido do que o do kubernetes, logo, não se deve ter a versão mais atualizada do docker pois o K8s pode não suportar.&lt;/p&gt;

&lt;h3 id=&quot;creating-a-single-control-plane-cluster-with-kubeadm&quot;&gt;Creating a single control-plane cluster with kubeadm&lt;/h3&gt;
&lt;p&gt;Os objetivos desta seção serão: Instalar um cluster Kubernetes com control-plane e instalar um puglin de rede usada para que os pods possam se comunicar entre si.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Esse processo  SÓ deve ser feito no MASTER! Apenas ele tem o Control Plane.&lt;/strong&gt;
Iniciar o control-plane node:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubeadm init
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Caso você use mais de uma interface de rede, é possível setar qual a interface você quer que seja usada nesta operação:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubeadm init --apiserver-advertise-address $(hostname -i)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note, os dois comandos são equivalentes caso você tenha apenas uma interface de rede!
A saída deste comando (kubeadmn init) deve ser similar a mostrada abaixo:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/insk8s_1.png&quot; alt=&quot;Saída esperada do kubeadm init&quot; style=&quot;width:650px;height:250px;&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;Figura 1 - Saída esperada do kubeadm init. Fonte: Autoria própia &lt;/figcaption&gt;
&lt;/p&gt;

&lt;p&gt;A partir de agora, você tem duas opções para fazer iniciar seu cluster:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Como um usuário comum.
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Como root.
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;export KUBECONFIG=/etc/kubernetes/admin.conf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;O kubeadm join… mostrado acima, é o comando utilizado para ingressar seus workers no seu cluster.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Na parte de rede, existem plugins que permitem criar redes virtuais ou rede de overlay. Eles criam túneis entre os nodes, eles permitem que um pod que está no node A consiga se comunicar com um pode no node B, por exemplo.&lt;/p&gt;

&lt;p&gt;Na documentação oficial, vimos alguns dos diferentes plugins disponíveis. Aqui foi usado o Weave, conforme está mostrado abaixo:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl apply -f &quot;https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Para passar o tráfego IPv4 para as cadeias de iptables, se faz necessário:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sysctl net.bridge.bridge-nf-call-iptables=1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Depois que o pod network foi instalado, você pode confirmar se o pod CoreDNS está Running na saída do:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; kubectl get pods --all-namespaces
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/insk8s_2.png&quot; alt=&quot;Saída esperada do kubectl get pods --all-namespaces (com alguns pods iniciando)&quot; style=&quot;width:620px;height:210px;&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;Figura 2 - Saída esperada do kubectl get pods --all-namespaces (com alguns pods iniciando). Fonte: Autoria própia &lt;/figcaption&gt;
&lt;/p&gt;

&lt;p&gt;Como mostrado na Figura 2, assim que se iniciam os pods ficam no status Pending para depois passar para Running.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/insk8s_3.png&quot; alt=&quot;Saída esperada do kubectl get pods --all-namespaces (com todos os pods prontos)&quot; style=&quot;width:650px;height:220px;&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;Figura 3 - Saída esperada do kubectl get pods --all-namespaces (com todos os pods prontos). Fonte: Autoria própia &lt;/figcaption&gt;
&lt;/p&gt;

&lt;p&gt;Após todos subirem corretamente, você pode verificar que os nodes.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get nodes
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/insk8s_4.png&quot; alt=&quot;Saída esperada do kubectl get nodes&quot; style=&quot;width:300px;height:80px;&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;Figura 4.1 - Saída esperada do kubectl get nodes, você deverá ver essa! Depois que adicionar os nodes, verá a Figura 4.2. Fonte: Autoria própia &lt;/figcaption&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/insk8s_5.png&quot; alt=&quot;Saída esperada do kubectl get nodes&quot; style=&quot;width:320px;height:100px;&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;Figura 4.2 - Saída esperada do kubectl get nodes com os workers adicionais. Fonte: Autoria própia &lt;/figcaption&gt;
&lt;/p&gt;

&lt;h3 id=&quot;joining-your-nodes&quot;&gt;Joining your nodes&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Esse processo deve ser feito nos workers!&lt;/strong&gt;
Aqui, você já deu o comando kubeadm init e já tem a saída, que inclue o kubeadm join, como no &lt;strong&gt;EXEMPLO&lt;/strong&gt; abaixo. Se atente que você deve usar o kubeadm join que você obteve, não o do exemplo.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; kubeadm join 10.128.0.3:6443 --token c3y122.omruf7ogbo4x34d7 \
    --discovery-token-ca-cert-hash sha256:4541d7661bdf610cca14500f00425de412e7773e8962c3ac0b218717bacce88c
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Após pegar este comando, você deve inseri-lo nas instâncias que serão seus workers.&lt;/p&gt;

&lt;p&gt;Abaixo é mostrado um exemplo dessa inserção, porém temos como retorno o seguinte erro: [ERROR FileContent–proc-sys-net-ipv4-ip_forward]: /proc/sys/net/ipv4/ip_forward contents are not set to 1.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/insk8s_6.png&quot; alt=&quot;Erro do kubeadm join&quot; style=&quot;width:1000px;height:115px;&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;Figura 5 - Erro do kubeadm join. Fonte: Autoria própia &lt;/figcaption&gt;
&lt;/p&gt;

&lt;p&gt;Após esse erro foi verificado que kubelet não estava funcionando e não foi possível reiniciá-lo.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/insk8s_7.png&quot; alt=&quot;Service kubelet não iniciado&quot; style=&quot;width:680px;height:270px;&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;Figura 6 - Service kubelet não iniciado. Fonte: Autoria própia &lt;/figcaption&gt;
&lt;/p&gt;

&lt;p&gt;Para solucionar foi necessário restartar o docker nos workers:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; systemctl restart docker 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Depois disso, o kubeadm join funcionou como o esperado e o kubelet subiu automaticamente.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/insk8s_8.png&quot; alt=&quot;Service kubelet iniciado&quot; style=&quot;width:1000px;height:350px;&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;Figura 7 - Service kubelet iniciado. Fonte: Autoria própia &lt;/figcaption&gt;
&lt;/p&gt;

&lt;p&gt;Caso você não tenho mais o token, é possível gerar outro. Todos os comandos relacionados a listagem/geração de tokens são feitas no master.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; kubeadm token list 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Os tokens expiram após 24 horas, caso você deseje gerar outro token:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubeadm token create 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;A saída será semelhante a isto:  5didvk.d09sbcov8ph2amjw
Caso você não tenha o valor do &lt;strong&gt;–discovery-token-ca-cert-hash&lt;/strong&gt;, você pode conseguir por meio desse comando:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&amp;gt;/dev/null | \
   openssl dgst -sha256 -hex | sed 's/^.* //'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Agora é só juntar as saídas anteriores e formar o comando completo do kubeadm join.&lt;/p&gt;

&lt;h3 id=&quot;remove-the-node&quot;&gt;Remove the node&lt;/h3&gt;
&lt;p&gt;Caso deseje remover um node, para fazer os comandos listados abaixo:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl drain &amp;lt;node name&amp;gt; --delete-local-data --force --ignore-daemonsets

kubectl delete node &amp;lt;node name&amp;gt;

kubeadm reset
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;comandos-interessantes&quot;&gt;Comandos interessantes&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Mostrar detalhes do Node com nome master
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl describe node master
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Mostrar algumas informações do cluster
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl cluster-info
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/insk8s_9.png&quot; alt=&quot; Informações do cluster&quot; style=&quot;width:800px;height:90px;&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;Figura 8 -  Informações do cluster. Fonte: Autoria própia &lt;/figcaption&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Mostrar status dos componentes
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get componentstatuses
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/insk8s_10.png&quot; alt=&quot; Informações dos componentes&quot; style=&quot;width:470px;height:70px;&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;Figura 9 -  Informações dos componentes. Fonte: Autoria própia &lt;/figcaption&gt;
&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h4 id=&quot;referências&quot;&gt;Referências&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;NSIGHT, UFC. Introdução a Kubernetes. Disponível em:&lt;a href=&quot;https://docs.google.com/presentation/d/1weqpBWa9FNjKc1ugCUIpwYYquvoIOFbUvEcZ9ZYapAg/edit#slide=id.g654e6a820d_0_553&quot;&gt;https://docs.google.com/presentation/d/1weqpBWa9FNjKc1ugCUIpwYYquvoIOFbUvEcZ9ZYapAg/edit#slide=id.g654e6a820d_0_553&lt;/a&gt;.Acessado em: 07 mar. 2020.&lt;/li&gt;
  &lt;li&gt;kubernets. Production environment. Disponível em:&lt;a href=&quot;https://kubernetes.io/docs/setup/production-environment&quot;&gt;https://kubernetes.io/docs/setup/production-environment&lt;/a&gt;. Acessado em: 07 mar. 2020.&lt;/li&gt;
  &lt;li&gt;Google Cloud. Como usar regras de firewall. Disponível em:&lt;a href=&quot;https://cloud.google.com/vpc/docs/using-firewalls?hl=pt-br&quot;&gt;https://cloud.google.com/vpc/docs/using-firewalls?hl=pt-br&lt;/a&gt;. Acessado em: 07 mar. 2020.&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Ana Amarante</name></author><summary type="html">Este documento foi desenvolvido em março de 2020, durante a disciplina DevOps na Residência em tecnologia da informação aplicada à área jurídica - turma JF/IMD, na área de especialidade de Redes e Infraestrutura, ministrado pelo Prof Me. André Solino.</summary></entry><entry><title type="html">Instalar o minikube em uma instância Gcloud</title><link href="http://localhost:4000/jekyll/update/2020/10/20/K8s-Minikube.html" rel="alternate" type="text/html" title="Instalar o minikube em uma instância Gcloud" /><published>2020-10-20T17:06:38-03:00</published><updated>2020-10-20T17:06:38-03:00</updated><id>http://localhost:4000/jekyll/update/2020/10/20/K8s-Minikube</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2020/10/20/K8s-Minikube.html">&lt;p&gt;Este documento foi desenvolvido em &lt;strong&gt;março de 2020&lt;/strong&gt;, durante a disciplina DevOps na Residência em tecnologia da informação aplicada à área jurídica - turma JF/IMD, na área de especialidade de Redes e Infraestrutura, ministrado pelo Prof Me. André Solino.&lt;/p&gt;

&lt;p&gt;O objetivo deste tutorial é a instalação do minikube numa instância da Google Cloud Platform. 
Este tutorial abordará desde o início a criação do projeto na Google Cloud Platform até a finalização da instalação do minikube. Os tópicos detalhados estão listados abaixo:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Definição teórica&lt;/li&gt;
  &lt;li&gt;Criação do projeto&lt;/li&gt;
  &lt;li&gt;Criação da instância de VM&lt;/li&gt;
  &lt;li&gt;Instalação do kubectl&lt;/li&gt;
  &lt;li&gt;Instalação do minikube&lt;/li&gt;
  &lt;li&gt;Deploy&lt;/li&gt;
  &lt;li&gt;Referências&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;definição-teórica&quot;&gt;Definição teórica&lt;/h2&gt;
&lt;p&gt;Mas afinal, o que é minikube?&lt;/p&gt;

&lt;p&gt;Primeiro, vamos falar do Kubernets, ele foi criado e desenvolvido pelos engenheiros da Google, uma das pioneiras no desenvolvimento da tecnologia de containers. Em 2015 o Kubernetes foi doado para a “Cloud Native Computing foundation” e Linux Foundations, e se tornou um projeto Open Source.&lt;/p&gt;

&lt;p&gt;Sua estrutura conta com o Master e os Workes. Onde o primeiro é responsável por controlar os nós do Kubernetes e os Workers são hosts do cluster (nós ou nodes), são as máquinas que realizam as tarefas solicitadas. Essa organização é bem parecida com o Docker Swarm.&lt;/p&gt;

&lt;p&gt;Apesar dessa semelhança o setup do Kubernets é mais complexo do que o do Docker Swarm. Mesmo para um ambiente de testes, o mínimo exigido eram cinco máquinas para simular um cluster com três Masters e dois Workes/Nós. Para melhorar isso, a comunidade criou o minikube, que permite subir rápido um ambiente, no qual é possível conhecer e ver o Kubernetes funcionando.&lt;/p&gt;

&lt;p&gt;O minikube simula um cluster real de um único nó, ou seja todos os processos e serviços que deveriam rodar no master e no slave rodam no node do minkube.&lt;/p&gt;

&lt;h2 id=&quot;criação-do-projeto&quot;&gt;Criação do projeto&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Acessar https://console.cloud.google.com/&lt;/li&gt;
  &lt;li&gt;Clique em “Selecione um projeto” no canto superior esquerdo.&lt;/li&gt;
  &lt;li&gt;Clique em “Novo projeto”. Digite o nome do projeto e finalize esta etapa.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;criação-da-instância-de-vm&quot;&gt;Criação da Instância de VM&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;No terminal digite $ssh-keygen no computador que será usado para acessar a instância de VM.&lt;/li&gt;
  &lt;li&gt;Clique em “Compute Engine’’, na seção “Computação’’. Essa opção é encontrada no menu de navegação, localizado na lateral esquerda.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Caso não veja esse menu ele está oculto, clique nas três barras no canto superior esquerdo que o menu será mostrado.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/google_platform.png&quot; alt=&quot;Barras que ocultam / mostram o menu de navegação&quot; style=&quot;width:170px;height:40px;&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;Figura 1 - Google Cloud Platform. Fonte: console.cloud.google.com &lt;/figcaption&gt;
&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A mensagem que o Google Compute Engine está sendo preparado será exibida, poderá levar alguns minutos e após isso será possível criar a instância.&lt;/li&gt;
  &lt;li&gt;Clique em “Criar’’, esta opção abre uma série de opções que podem ser personalizadas de acordo com o objetivo. Abaixo será mostrado apenas o que precisa ser alterado, as demais opções continuam com o valor padrão.
    &lt;ul&gt;
      &lt;li&gt;Nome: minikube.&lt;/li&gt;
      &lt;li&gt;Tipo de máquina: n1-standard-2&lt;/li&gt;
      &lt;li&gt;Disco de inicialização: CentOS 7.&lt;/li&gt;
      &lt;li&gt;Clicar em: “Gerenciamento, segurança, discos, rede, locatário único“.
        &lt;ul&gt;
          &lt;li&gt;Clicar na aba “Segurança’’.&lt;/li&gt;
          &lt;li&gt;Inserir a chave SSH pública gerada no ponto 4. Sempre é possível editar essas chaves, inserir novas ou remover alguma já configurada.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Clicar em “Criar’’ para finalizar esta etapa.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;instalação-do-kubectl&quot;&gt;Instalação do kubectl&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Ligar a instância e acessá-la.
    &lt;ul&gt;
      &lt;li&gt;Selecione a instância que foi criada, em seguida clique em “Iniciar’’ na parte superior.&lt;/li&gt;
      &lt;li&gt;No terminal digite $ssh user@IPexterno, esse IP é mostrado depois que a instância foi iniciada.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Instalar kubectl.
    &lt;ul&gt;
      &lt;li&gt;Download do último release:
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;Conceda a permissão de execução para o binário do kubectl
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; chmod +x ./kubectl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;Mova o binário para o PATH indicado:
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; sudo mv ./kubectl /usr/local/bin/kubectl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;Teste para verificar se a instalação ocorreu corretamente e na versão indicada:
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; kubectl version --client
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;A saída esperada está mostrada abaixo:&lt;/li&gt;
    &lt;/ul&gt;
    &lt;p align=&quot;center&quot;&gt;
 &lt;img src=&quot;/images/minikube_1.png&quot; alt=&quot;Saída do comando para verificação do kubectl&quot; style=&quot;width:900px;height:75px;&quot; /&gt;
 &lt;figcaption align=&quot;center&quot;&gt;Figura 2 - Saída do comando para verificação do kubectl. Fonte: Autoria própia &lt;/figcaption&gt;
 &lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;instalação-do-minikube&quot;&gt;Instalação do minikube&lt;/h2&gt;
&lt;p&gt;A documentação oficial foi usada como base, o link está no final deste documento.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Download do binário:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 \
  &amp;amp;&amp;amp; chmod +x minikube
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Adicionar o binário para o PATH indicado:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo mkdir -p /usr/local/bin/
sudo install minikube /usr/local/bin/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;O minikube precisa do docker:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo yum install docker
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;O –vm-driver=none precisa da permissão de root:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo su -
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;Como estamos trabalhando em um ambiente de testes, iremos parar o firewalld e desabilitar o SELinux. Caso você esteja em um ambiente de produção é importante rever a documentação desses serviços para que eles fiquem habilitados e não abram falhas de segurança na sua implementação.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Parando o firewalld:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;systemctl stop firewalld
systemctl disable firewalld
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Parando o SELinux:  em seguida alterar a linha “SELINUX=enforcing’ confirme está mostrado abaixo
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vim /etc/sysconfig/selinux
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;ul&gt;
      &lt;li&gt;Altere a linha “SELINUX=enforcing” para:
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;SELINUX=disabled
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Agora é necessário reiniciar a máquina para que a alteração do SELinux seja feita.&lt;/li&gt;
  &lt;li&gt;Caso você deseje conferir, após reiniciar a máquina digite $sestatus, você verá que o SELinux foi desabilitado.&lt;/li&gt;
  &lt;li&gt;Verifique a instalação iniciando o minikube.  &lt;br /&gt;
Execute o seguinte comando no seu terminal para concluir a criação do cluster, um cluster de nó único será criado. Observe que esse comando pode demorar um pouco:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;minikube start --vm-driver=none
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Após a finalização do minikube start, é hora de checar se o minikube está rodando conforme o esperado:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;minikube status
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;ul&gt;
      &lt;li&gt;A saída esperada está mostrada abaixo:&lt;/li&gt;
    &lt;/ul&gt;
    &lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/minikube_2.png&quot; alt=&quot;Saída esperada do comando para verificação do minikube&quot; style=&quot;width:300px;height:100px;&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;Figura 3 - Saída esperada do comando para verificação do minikube. Fonte: Autoria própia &lt;/figcaption&gt;
&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Como já foi dito o minikube simula um cluster real de um único nó, é possível vê esse comportamento por meio do comando $kubectl get nodes. A saída deve ser conforme está mostrado abaixo.
    &lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/minikube_3.png&quot; alt=&quot;Saída esperada do comando para verificação de nodes do kubernetes&quot; style=&quot;width:290px;height:70px;&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;Figura 4 - Saída esperada do comando para verificação de nodes do kubernetes. Fonte: Autoria própia &lt;/figcaption&gt;
&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;deployment&quot;&gt;Deployment&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Agora, vamos subir um deployment do ngnix para rodar em nosso minikube.
    &lt;ul&gt;
      &lt;li&gt;Modo imperativo
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl run ngnix --generator=run-pod/v1 --image ngnix
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;Modo declarativo
```
apiVersion: v1
kind: Pod
metadata:
name: ngnix-pod
spec:
containers:
        &lt;ul&gt;
          &lt;li&gt;name: ngnix-container
image: ngnix
```&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Vamos verificar o deployment, por meio do comando $kubectl get deployment. A saída deve ser similar a mostrada abaixo:
    &lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/minikube_4.png&quot; alt=&quot;Saída esperada da verificação do deployment&quot; style=&quot;width:320px;height:60px;&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;Figura 5 - Saída esperada da verificação do deployment. Fonte: Autoria própia &lt;/figcaption&gt;
&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Verificar os pods:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get pods
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;ul&gt;
      &lt;li&gt;A saída esperada está mostrada abaixo:&lt;/li&gt;
    &lt;/ul&gt;
    &lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/minikube_5.png&quot; alt=&quot;Saída esperada da verificação dos pods&quot; style=&quot;width:400px;height:70px;&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;Figura6 - Saída esperada da verificação dos pods. Fonte: Autoria própia &lt;/figcaption&gt;
&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h4 id=&quot;referências&quot;&gt;Referências&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Tudo o que você precisa saber sobre Kubernetes – Parte 0. iMasters. Acessado em 23 de fev. 2020. Disponível em: &lt;a href=&quot;https://imasters.com.br/desenvolvimento/tudo-o-que-voce-precisa-saber-sobre-kubernetes-parte-01&quot;&gt;https://imasters.com.br/desenvolvimento/tudo-o-que-voce-precisa-saber-sobre-kubernetes-parte-01&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Install Minikube. Kubernetes. Acessado em 23 de fev. 2020. Disponível em: &lt;a href=&quot;https://kubernetes.io/docs/tasks/tools/install-minikube/&quot;&gt;https://kubernetes.io/docs/tasks/tools/install-minikube/&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;01 - O que é o KUBERNETES e INSTALANDO o MINIKUBE. Descomplicando o Kubernetes. Acessado em 23 de fev. 2020. Disponível em: &lt;a href=&quot;https://www.youtube.com/watch?v=pV0nkr61XP8&quot;&gt;https://www.youtube.com/watch?v=pV0nkr61XP8&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Ana Amarante</name></author><summary type="html">Este documento foi desenvolvido em março de 2020, durante a disciplina DevOps na Residência em tecnologia da informação aplicada à área jurídica - turma JF/IMD, na área de especialidade de Redes e Infraestrutura, ministrado pelo Prof Me. André Solino.</summary></entry><entry><title type="html">Conceitos básicos do Kubernetes</title><link href="http://localhost:4000/jekyll/update/2020/04/12/K8s-Conceitos-introdut%C3%B3rios.html" rel="alternate" type="text/html" title="Conceitos básicos do Kubernetes" /><published>2020-04-12T17:06:38-03:00</published><updated>2020-04-12T17:06:38-03:00</updated><id>http://localhost:4000/jekyll/update/2020/04/12/K8s-Conceitos-%20introdut%C3%B3rios</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2020/04/12/K8s-Conceitos-introdut%C3%B3rios.html">&lt;h2 id=&quot;introdução&quot;&gt;Introdução&lt;/h2&gt;

&lt;p&gt;Este documento foi desenvolvido durante a disciplina DevOps na Residência em TI aplicada à área jurídica - turma JF/IMD, na especialidade de Redes e Infraestrutura. O docente foi o Prof Me. André Solino.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/kubernetes_logo.png&quot; alt=&quot;Logotipo do kubernetes&quot; style=&quot;width:200px;height:190px;&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;Figura 1 - Logotipo do kubernetes. Fonte: kubernetes.io.&lt;/figcaption&gt;
&lt;/p&gt;

&lt;p&gt;Na figura 1, podemos vê à logotipo do Kubernetes (conhecido também como K8s), que sugere um leme de um navio. Mas sabemos que um navio pode levar praticamente qualquer coisa, porém em larga escala, seja lá o que for carregado dentro de um navio, está armazenado dentro de um container.&lt;/p&gt;

&lt;p&gt;Uma informação importante é que a palavra Kubernetes vem da palavra grega Kuvernetes, que representa a pessoa que pilota o navio.&lt;/p&gt;

&lt;p&gt;Com a inferência correta, já sabemos que o Kubernetes é responsável por fazer a orquestração e gerenciamento de containers! Ele é um sistema de orquestração open-source que automatiza a implantação, o dimensionamento e a gestão de aplicações em containers.&lt;/p&gt;

&lt;p&gt;O Kubernetes faz o gerenciamento do cluster, que é composto por vários containers. O Docker é a tecnologia mais utilizada para  provisionar os containers O próprio Docker tem seu orquestrador, o Docker Swarm, porém ele não é tão maduro quanto o Kubernetes.&lt;/p&gt;

&lt;h2 id=&quot;breve-história&quot;&gt;Breve história&lt;/h2&gt;
&lt;p&gt;O K8s (trocadilho com o nome Kubernetes: k + 8 caracteres + s) é inscrito em GO, foi desenvolvido pela Google, para gerência dos serviços da G Suite. Em 2015 o Kubernetes foi doado para a “Cloud Native Computing foundation” e Linux Foundations, e se tornou um projeto Open Source, hoje é mantido pela comunidade. Isso é mais um incentivo a sua popularidade e grande utilização, visto que por já ser da comunidade, não fica restrito a nenhuma alteração drástica partindo da empresa que detivesse essa ferramenta.&lt;/p&gt;

&lt;h2 id=&quot;conceitos-importantes&quot;&gt;Conceitos importantes&lt;/h2&gt;
&lt;p&gt;Quando se implanta o Kubernetes, que é o &lt;strong&gt;deploy&lt;/strong&gt; (esse conceito será detalhado abaixo), você passa a ter um &lt;strong&gt;cluster&lt;/strong&gt;. Que é composto pelo  &lt;strong&gt;master&lt;/strong&gt; e os &lt;strong&gt;workers&lt;/strong&gt;, também conhecido como &lt;strong&gt;nodes&lt;/strong&gt; ou &lt;strong&gt;minions&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;O master é responsável por controlar os nodes do Kubernetes, que são quem fazem as tarefas dentro dos containers. Cada cluster tem pelo menos um node ou worker. 
Dentro dos workers ficam os &lt;strong&gt;pods&lt;/strong&gt;. Em um node é possível ter nenhum, um ou vários pods, a depender da sua aplicação.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Master: Executa o Control Plane.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Node: Executa os containers que estão dentro dos pods. É a menor unidade de hardware de computação no Kubernetes.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Também existem outros componentes que estão dentro do cluster, são eles:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;kubeadm: Automatiza parte do processo de criação do cluster.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;kubectl: Interface de linha de comando do K8s, ele opera o cluster gerenciando suas interações. Ainda temos o kubelet, mas ele será detalhado nos componentes do node.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Os pods são unidade do Kubernetes, dentro dele pode ter um ou mais containers e uma unidade de armazenamento. É característica dos pods serem voláteis e cada um tem um IP e id únicos. Eles podem ter diferentes estados:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Pending: Estado inicial do pod.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Running: Em execução.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Success: Pod concluiu a tarefa e morreu.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Unknown: Estado não reconhecido, o linux retornou qualquer coisa diferente de 0.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Dentro dos pods, temos os &lt;strong&gt;services&lt;/strong&gt;, que são uma maneira abstrata de expor uma aplicação em execução em um conjunto de pods como um serviço de rede.&lt;/p&gt;

&lt;p&gt;O acesso aos clusters, é feito por meio do &lt;strong&gt;ingress&lt;/strong&gt;. É uma coleção de regras de roteamento, responsáveis por estabelecer como usuários externos acessam serviços que estão em execução em um cluster Kubernetes.&lt;/p&gt;

&lt;p&gt;A rede no Kubernetes é configurada por meio de plugins que permitem criar redes virtuais ou rede de overlay. Eles criam túneis criptografados entre os nodes, que permitem que toda comunicação ocorra. Ou seja, um pod que está no node A consegue se comunicar com um pod no node B.&lt;/p&gt;

&lt;p&gt;Existem vários plugins, como Flannel ou Weave, porém a lista de opções é bem maior e pode ser conferida &lt;a href=&quot;https://kubernetes.io/docs/concepts/cluster-administration/networking/&quot;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Visando organizar objetos dentro do cluster, temos o conceito de namespace, que pode ser entendido como uma divisão lógica. Por padrão, kubectl interage com o namespace padrão (default).&lt;/p&gt;

&lt;p&gt;Agora, que já temos alguns conceitos esclarecidos, vamos saber mais sobre a estrutura do Kubernetes.&lt;/p&gt;

&lt;h3 id=&quot;control-plane-e-nodes&quot;&gt;Control Plane e Nodes&lt;/h3&gt;
&lt;p&gt;O K8s possui um  software conhecido como &lt;strong&gt;control plane&lt;/strong&gt;, nele é decido quando e onde executar os pods, além de gerenciar o roteamento do tráfego e escalar os pods de acordo com a utilização ou outras métricas definidas.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/Kubernetes_controlplane.png&quot; alt=&quot; Control Plane&quot; style=&quot;width:750px;height:400px;&quot; /&gt;
&lt;figcaption align=&quot;center&quot;&gt;Figura 2 - Estrututura do kubernetes. Fonte: kubernetes.io.&lt;/figcaption&gt;
&lt;/p&gt;

&lt;p&gt;Os componentes do Control Plane tomam decisões gerais sobre o cluster, além de detectar e responder a eventos do cluster, como iniciar um novo pod quando uma réplica de um &lt;strong&gt;deployment&lt;/strong&gt; não inicia corretamente, por exemplo.&lt;br /&gt;
Um deployment é uma implantação de uma aplicação dentro do Kubernetes, por exemplo um deploy do ngnix, vai subir um ngnix. Ao configurar um deploy, é possível setar quantos pods se deseja iniciar para determinada aplicação, a essa abstração damos o nome de réplicas.&lt;/p&gt;

&lt;p&gt;Como é visto na figura 2, temos alguns componentes do Control Plane, que são:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;etcd: É uma base de dados de chave valor, que armazena os dados de configuração do cluster e o estado do cluster.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;kube-apiserver: Fornece API kubernetes usando Jason, baseado em REST. Os estados de objetos da API são armazenados no etcd, e o kubectl usa o kube-api-server para se comunicar com o cluster.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;kube-controller-manager: Controles responsáveis por monitorar continuamente o estado do cluster por meio da apiserver. Além de fazer mudanças de estado no cluster, alguns exemplos são o replication controller, endpoints controller, namespace controller, and serviceaccounts controller.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;cloud-controller-manager: Um daemon que incorpora loops de controle específicos da nuvem. Os provedores de nuvem se desenvolvem em um ritmo diferente do Kubernetes, sendo necessário o binário cloud-controller-manager para permitir que esses provedores abstraiam o código que seria para o kube-controller-manager.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;kube-scheduler: Responsável por executar as tarefas de agendamento, como execução de containers nos nodes com base na disponibilidade de recursos. Quando um pod está no estado pending, quem decide para onde o pod irá será este componente. É possível interferir nesta política, por exemplo indicando a máquina para onde os pods devem ir preferencialmente.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Agora, veremos os componentes dos nodes:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;kube-proxy: Quando se cria pods, ele pode ir pra qualquer máquina e um pod precisa se comunicar com o outro, ou seja, é necessário a comunicação entre os nodes. O kube-proxy é responsável por lidar com as regras de firewall.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;kubelet: Faz a interface com o docker, atua como agente em cada node. Lida com a execução dos pods. Sempre conversa com o kube-apiserver.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Para que o post não fique muito longo, não foram apresentados mais conceitos. Eles são introdutórios, porém importantes para entender todo o resto do Kubernetes!&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h4 id=&quot;referências&quot;&gt;Referências&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Tudo o que você precisa saber sobre Kubernetes – Parte 0. iMasters.  Disponível em: &lt;a href=&quot;https://imasters.com.br/desenvolvimento/tudo-o-que-voce-precisa-saber-sobre-ku,bernetes-parte-01&quot;&gt;https://imasters.com.br/desenvolvimento/tudo-o-que-voce-precisa-saber-sobre-ku,bernetes-parte-01&lt;/a&gt;. Acessado em 23 de fev. 2020.&lt;/li&gt;
  &lt;li&gt;What is Kubernetes. Kubernetes. Disponível em: &lt;a href=&quot;https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/&quot;&gt;https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/&lt;/a&gt;. Acessado em 23 de fev. 2020.&lt;/li&gt;
  &lt;li&gt;Kubernetes Components. Kubernetes. Disponível em: &lt;a href=&quot;https://kubernetes.io/docs/concepts/overview/components/&quot;&gt;https://kubernetes.io/docs/concepts/overview/components/&lt;/a&gt;. Acessado em 23 de fev. 2020.&lt;/li&gt;
  &lt;li&gt;Alta disponibilidade com Kubernetes na prática. Tech@Grupo ZAP. Disponível em: &lt;a href=&quot;https://medium.com/tech-grupozap/alta-disponibilidade-com-kubernetes-na-pr%C3%A1tica-b9cf4261d2f7&quot;&gt;https://medium.com/tech-grupozap/alta-disponibilidade-com-kubernetes-na-pr%C3%A1tica-b9cf4261d2f7&lt;/a&gt;. Acessado em 23 de fev. 2020.&lt;/li&gt;
  &lt;li&gt;Kubernetes na AWS. AWS. Disponível em: &lt;a href=&quot;https://aws.amazon.com/pt/kubernetes/&quot;&gt;https://aws.amazon.com/pt/kubernetes/&lt;/a&gt;. Acessado em 23 de fev. 2020.&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Ana Amarante</name></author><summary type="html">Introdução</summary></entry></feed>